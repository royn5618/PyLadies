{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfwcnxOACciawQ/W51HiEo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royn5618/Talks_Resources/blob/main/PyLadies/TextDataScrape_ThreeQuickWays/Quick_Easy_PythonModules_TextData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wikipedia\n",
        "\n",
        "GitHub: https://github.com/goldsmith/Wikipedia\n",
        "\n",
        "Wikipedia is a Python library that makes it easy to access and parse data from Wikipedia."
      ],
      "metadata": {
        "id": "94rryRB9bNNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "UPvvjOlobm9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "results_list = wikipedia.search(\"pyladies\") # Enter you search term\n",
        "results_list"
      ],
      "metadata": {
        "id": "fAJWnp2tbM8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_list = wikipedia.search(\"pyladies AND python\") # Enter you search term\n",
        "results_list"
      ],
      "metadata": {
        "id": "zwm1_DVxcf6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_list = wikipedia.search(\"pyladies OR python\")\n",
        "results_list"
      ],
      "metadata": {
        "id": "qbvQlSHCcbIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_list = wikipedia.search(\"python\")\n",
        "results_list"
      ],
      "metadata": {
        "id": "b-Zp_5Utdc8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_list = wikipedia.search(\"python AND pyladies\")\n",
        "results_list"
      ],
      "metadata": {
        "id": "yJgEgbFAdg7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_page_obj = wikipedia.page('PyLadies')\n",
        "wiki_page_obj"
      ],
      "metadata": {
        "id": "gLU-15xtuCb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_page_obj.content"
      ],
      "metadata": {
        "id": "tvBQfUUMuS7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_page_obj.url"
      ],
      "metadata": {
        "id": "Dc5f3kpJuUgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_page_obj.links"
      ],
      "metadata": {
        "id": "I4wW6uO3uW_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_search_results = []"
      ],
      "metadata": {
        "id": "CPQiolVebwoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMomvQcCYARW"
      },
      "outputs": [],
      "source": [
        "for each_result in results_list:\n",
        "    wiki_page_result = {}\n",
        "    wiki_page_obj = wikipedia.page(each_result)\n",
        "    wiki_page_result['title'] = wiki_page_obj.title\n",
        "    wiki_page_result['content'] = wiki_page_obj.content\n",
        "    wiki_page_result['url'] = wiki_page_obj.url\n",
        "    wiki_page_result['links'] = wiki_page_obj.links\n",
        "    wiki_search_results.append(wiki_page_result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(wiki_search_results)"
      ],
      "metadata": {
        "id": "3JfIxVTJua4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Wrapper for Twitter using SNScrape\n",
        "\n",
        "GitHub: https://github.com/JustAnotherArchivist/snscrape\n",
        "\n",
        "Tutorial: https://betterprogramming.pub/how-to-scrape-tweets-with-snscrape-90124ed006af"
      ],
      "metadata": {
        "id": "Y0Oifldvu0Gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snscrape"
      ],
      "metadata": {
        "id": "xGmQ4uMDuz0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import snscrape.modules.twitter as sntwitter"
      ],
      "metadata": {
        "id": "vH_mCYnGw56x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sntwitter.TwitterSearchScraper('from:PyLadiesDub').get_items()"
      ],
      "metadata": {
        "id": "o_RXDS5jxFVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in sntwitter.TwitterSearchScraper('from:PyLadiesDub').get_items():\n",
        "    print(\"Tweet Date: \", tweet.date)\n",
        "    print(\"Tweet ID: \", tweet.id)\n",
        "    print(\"Tweet Content: \", tweet.content)\n",
        "    print(\"Twitter User: \", tweet.user.username)\n",
        "    break"
      ],
      "metadata": {
        "id": "-pdTsrPWxvks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in sntwitter.TwitterSearchScraper('#PyLadies #Hamburg').get_items():\n",
        "    print(\"Tweet Date: \", tweet.date)\n",
        "    print(\"Tweet ID: \", tweet.id)\n",
        "    print(\"Tweet Content: \", tweet.content)\n",
        "    print(\"Twitter User: \", tweet.user.username)\n",
        "    break"
      ],
      "metadata": {
        "id": "LbhOotM30BqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attributes Available:\n",
        "\n",
        "1. URL\n",
        "2. Date\n",
        "3. content\n",
        "4. id\n",
        "5. user\n",
        "  - username\n",
        "  - display name\n",
        "  - id\n",
        "  - description\n",
        "  - verified\n",
        "  - followersCount\n",
        "  - friendsCount\n",
        "  - location\n",
        "6. replyCount\n",
        "7. retweetCount\n",
        "8. likeCount\n",
        "9. media\n",
        "10. mentionedUsers\n",
        "11. Lang\n",
        "\n",
        "  Find a wider list on the tutorial by the author"
      ],
      "metadata": {
        "id": "Z0GWMKJuGJp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage of AND / OR for combinations of search terms\n",
        "\n",
        "for tweet in sntwitter.TwitterSearchScraper('#PyLadies OR #Hamburg').get_items():\n",
        "    print(\"Tweet Date: \", tweet.date)\n",
        "    print(\"Tweet ID: \", tweet.id)\n",
        "    print(\"Tweet Content: \", tweet.content)\n",
        "    print(\"Twitter User: \", tweet.user.username)\n",
        "    break"
      ],
      "metadata": {
        "id": "ZjhthJgG0DbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Sample Code'''\n",
        "\n",
        "# Creating list to append tweet data to\n",
        "tweets_list1 = []\n",
        "maxTweets = 50\n",
        "# Using TwitterSearchScraper to scrape data \n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:PyLadiesDub since:2023-01-01 until:2023-02-01').get_items()):\n",
        "    if i > maxTweets:\n",
        "        break\n",
        "    tweets_list1.append([tweet.date, tweet.id, tweet.rawContent, tweet.user.username])\n",
        "\n",
        "df = pd.DataFrame(tweets_list1, columns = ['date', 'id', 'content', 'username'])\n",
        "df.index = pd.to_datetime(df['date'])\n",
        "df.drop(columns='date', inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nIBY5u9txOEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Jha7iGHqzfQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pattern for Twitter and OOB Applied-NLP \n",
        "\n",
        "GitHub: https://github.com/clips/pattern"
      ],
      "metadata": {
        "id": "_pe1mNxdzmft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pattern"
      ],
      "metadata": {
        "id": "kncCNZORy6Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pattern.web import Twitter"
      ],
      "metadata": {
        "id": "t3z7P6Eh6tpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter = Twitter()"
      ],
      "metadata": {
        "id": "D4T6HlSg6zuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter.search('#pyladies', start=1, count=3)"
      ],
      "metadata": {
        "id": "FH4e3ZK463i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attributes Available:\n",
        "1. URL\n",
        "2. ID - Twitter has unique ids for each so you can link back to SnScraper Tweet ID as well\n",
        "3. Text - Also called content in SnScaper\n",
        "4. Language\n",
        "5. Author\n",
        "6. Date\n",
        "7. Profile\n"
      ],
      "metadata": {
        "id": "2ZAJc_9T7ECp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from pattern.en import tag\n",
        "from pattern.vector import KNN, count"
      ],
      "metadata": {
        "id": "5vx5KUEJuvE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNN()\n",
        "for tweet in twitter.search('#pyladies', start=1, count=20):\n",
        "  s = tweet.text.lower()\n",
        "  # print(s)\n",
        "  # print(\"---\")\n",
        "  p = 'dublin' in s and 'YES' or 'NO'\n",
        "  v = tag(s)\n",
        "  # for word,pos in v:\n",
        "  #   print(\"Word:\", word)\n",
        "  #   print(\"POS:\", pos)\n",
        "  #   # break\n",
        "  # print(\"---\")\n",
        "  v = [word for word, pos in v if pos == 'IN' or pos == 'JJ']\n",
        "  # print(v)\n",
        "  v = count(v)\n",
        "  print(\"---\")\n",
        "  print(v)\n",
        "  if v:\n",
        "    knn.train(v, type=p)"
      ],
      "metadata": {
        "id": "IZpHN-P674lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(knn.classify('sweet little burger'))\n",
        "print(knn.classify('I am at pyladies dublin'))"
      ],
      "metadata": {
        "id": "HL5NGy5NDjd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(knn.classify('It is a lovely day in dublin'))"
      ],
      "metadata": {
        "id": "NdSYOBtZD9sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(knn.classify('It is a lovely day in Munich'))"
      ],
      "metadata": {
        "id": "DBwyh6WWD8lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(knn.classify('It is a great day in Munich #dublin #munich #chicago'))"
      ],
      "metadata": {
        "id": "IAe91ni17zPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Q_qBIRbubrf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}